{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gpytorch\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer import MCMC, NUTS\n",
    "\n",
    "# Define the GP model for X(t) with a Matérn kernel\n",
    "class GPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood, nu=2.5):\n",
    "        super(GPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "            gpytorch.kernels.MaternKernel(nu=nu)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "# Helper function to calculate kernel derivatives for Xdot\n",
    "def kernel_derivative(kernel, x1, x2):\n",
    "    x1 = x1.unsqueeze(-1) if x1.dim() == 1 else x1\n",
    "    x2 = x2.unsqueeze(-1) if x2.dim() == 1 else x2\n",
    "    matern_kernel = kernel.base_kernel\n",
    "    lengthscale = matern_kernel.lengthscale.item()\n",
    "    \n",
    "    # Compute k(x1, x2) - Matern kernel without scaling\n",
    "    dist = torch.cdist(x1, x2, p=2)\n",
    "    nu = matern_kernel.nu\n",
    "    if nu == 0.5:\n",
    "        k_xx = torch.exp(-dist / lengthscale)\n",
    "    elif nu == 1.5:\n",
    "        k_xx = (1 + dist / lengthscale) * torch.exp(-dist / lengthscale)\n",
    "    elif nu == 2.5:\n",
    "        k_xx = (1 + (dist / lengthscale) + (dist**2) / (3 * lengthscale**2)) * torch.exp(-dist / lengthscale)\n",
    "    else:\n",
    "        raise NotImplementedError(\"Only Matérn kernels with ν=0.5, 1.5, and 2.5 are implemented\")\n",
    "\n",
    "    # Derivative w.r.t x2\n",
    "    k_xdot = -(x1 - x2.transpose(0, 1)) * k_xx / (lengthscale**2)\n",
    "    \n",
    "    # Second derivative for variance of the derivative\n",
    "    k_xdotxdot = (1 - (dist.pow(2) / (lengthscale**2))) * k_xx / (lengthscale**2)\n",
    "    \n",
    "    return k_xx, k_xdot, k_xdotxdot\n",
    "\n",
    "# Define the joint posterior model with non-Gaussian prior on theta and parameter mu for lengthscale\n",
    "def joint_posterior(train_x, train_y, test_x_tau, test_xdot, y_tau, f_t):\n",
    "    # Non-Gaussian prior on theta (Gamma prior)\n",
    "    theta = pyro.sample(\"theta\", dist.Gamma(2.0, 1.0))  # shape=2.0, rate=1.0\n",
    "    # Prior on mu (for lengthscale of the Matern kernel)\n",
    "    mu = pyro.sample(\"mu\", dist.Gamma(2.0, 1.0))  # shape=2.0, rate=1.0\n",
    "    \n",
    "    # Define likelihood and model with given theta and mu\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "    model = GPModel(train_x, train_y, likelihood)\n",
    "    model.covar_module.base_kernel.lengthscale = mu  # Set the lengthscale parameter\n",
    "\n",
    "    # Train the GP model with fixed theta and mu (manual training for demonstration)\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "    # Training loop\n",
    "    for i in range(50):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(train_x)\n",
    "        loss = -mll(output, train_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "\n",
    "    # Part 1: Compute p(X(I) = x(I)) as log probability\n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "        observed_pred = model(train_x)\n",
    "        marginal_log_prob_XI = mll(observed_pred, train_y)\n",
    "    \n",
    "    # Part 2: Compute p(Y(τ) = y(τ) | X(I) = x(I))\n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "        predictive_dist_y = model(test_x_tau)\n",
    "        log_prob_Y_given_XI = predictive_dist_y.log_prob(y_tau)\n",
    "    \n",
    "    # Part 3: Compute p(Xdot(t) = f(t) | X(I) = x(I), Y(τ) = y(τ))\n",
    "    K_xx, K_xdot, K_xdotxdot = kernel_derivative(model.covar_module, train_x, test_xdot)\n",
    "    K_xx_inv = torch.inverse(K_xx + 1e-6 * torch.eye(K_xx.size(0)))  # Regularize for stability\n",
    "\n",
    "    # Conditional mean and variance for Xdot given X(I) and Y(τ)\n",
    "    conditional_mean_Xdot = K_xdot.T @ K_xx_inv @ (train_y - model.mean_module(train_x))\n",
    "    conditional_variance_Xdot = K_xdotxdot - K_xdot.T @ K_xx_inv @ K_xdot\n",
    "    log_prob_Xdot_given_XI_Ytau = -0.5 * ((f_t - conditional_mean_Xdot)**2 / conditional_variance_Xdot).sum()\n",
    "    \n",
    "    # Sum of log probabilities\n",
    "    joint_log_prob = marginal_log_prob_XI + log_prob_Y_given_XI + log_prob_Xdot_given_XI_Ytau\n",
    "\n",
    "    # Return the negative log posterior (for optimization/sampling)\n",
    "    return -joint_log_prob\n",
    "\n",
    "# Define the MCMC sampling function with NUTS\n",
    "def run_mcmc(train_x, train_y, test_x_tau, test_xdot, y_tau, f_t, num_samples=1000, warmup_steps=200):\n",
    "    nuts_kernel = NUTS(joint_posterior)\n",
    "    mcmc = MCMC(nuts_kernel, num_samples=num_samples, warmup_steps=warmup_steps)\n",
    "    \n",
    "    mcmc.run(train_x, train_y, test_x_tau, test_xdot, y_tau, f_t)\n",
    "    posterior_samples = mcmc.get_samples()\n",
    "    \n",
    "    return posterior_samples\n",
    "\n",
    "# Define observation points and values\n",
    "train_x = torch.linspace(0, 10, 100)  # I: Observation points for X(t)\n",
    "train_y = torch.sin(train_x)  # Observed values x(I)\n",
    "test_x_tau = torch.tensor([5.5, 7.5])  # τ: New points for Y(τ)\n",
    "y_tau = torch.sin(test_x_tau)  # Observed values for Y(τ)\n",
    "test_xdot = torch.tensor([3.0])  # t: Point for Xdot(t)\n",
    "f_t = torch.tensor([0.5])  # Substitute with actual observed derivative\n",
    "\n",
    "# Run MCMC to sample from the joint posterior\n",
    "posterior_samples = run_mcmc(train_x, train_y, test_x_tau, test_xdot, y_tau, f_t)\n",
    "\n",
    "# Output samples\n",
    "print(\"Posterior Samples for Theta:\")\n",
    "print(posterior_samples['theta'])\n",
    "print(\"Posterior Samples for Mu (lengthscale):\")\n",
    "print(posterior_samples['mu'])\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
